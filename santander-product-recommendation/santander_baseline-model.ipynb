{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "np.random.seed(156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yshin/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/yshin/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "trn = pd.read_csv('train_ver2.csv')\n",
    "tst = pd.read_csv('test_ver2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**데이터 전처리** :\n",
    "\n",
    "- 제품 변수의 결측값을 0으로 대체 (정보 없음 = 해당 제품 미보유)\n",
    "- 훈련/테스트 데이터 통합 *테스트 데이터에 없는 제품 변수는 0\n",
    "- 범주형 데이터는 .factorize() 통해 label encoding\n",
    "- 수치형 데이터는 .unique()를 통해 특이값 처리, 정수형으로 변환\n",
    "- 학습에 사용할 변수 이름을 features 리스트로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제품 변수를 별도로 저장해 놓는다.\n",
    "prods = trn.columns[24:].tolist()\n",
    "\n",
    "# 제품 변수 결측값을 미리 0으로 대체\n",
    "trn[prods] = trn[prods].fillna(0.0).astype(np.int8)\n",
    "\n",
    "# 24개 제품 중 하나도 보유하지 않는 고객 데이터는 제거\n",
    "no_product = trn[prods].sum(axis=1) == 0\n",
    "trn = trn[~no_product]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터와 테스트 데이터를 통합\n",
    "for col in trn.columns[24:]:\n",
    "    tst[col] = 0\n",
    "df = pd.concat([trn, tst], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [] #학습에 사용할 변수 리스트\n",
    "\n",
    "## 범주형 변수: label encoding\n",
    "categorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', 'canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento']\n",
    "for col in categorical_cols:\n",
    "    df[col], _ = df[col].factorize(na_sentinel=-99)\n",
    "features += categorical_cols\n",
    "\n",
    "# 수치형 변수: 특이값과 결측값을 -99로 대체, 정수형으로 변환\n",
    "df['age'].replace(' NA', -99, inplace=True)\n",
    "df['age'] = df['age'].astype(np.int8)\n",
    "\n",
    "df['antiguedad'].replace('     NA', -99, inplace=True)\n",
    "df['antiguedad'] = df['antiguedad'].astype(np.int8)\n",
    "\n",
    "df['renta'].replace('         NA', -99, inplace=True)\n",
    "df['renta'].fillna(-99, inplace=True)\n",
    "df['renta'] = df['renta'].astype(float).astype(np.int8)\n",
    "\n",
    "df['indrel_1mes'].replace('P', 5, inplace=True)\n",
    "df['indrel_1mes'].fillna(-99, inplace=True)\n",
    "df['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)\n",
    "\n",
    "features += ['age','antiguedad','renta','ind_nuevo','indrel','indrel_1mes','ind_actividad_cliente']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**피쳐 엔지니어링** :\n",
    "- 24개의 고객 변수와 4개의 날짜 변수 기반 파생 변수, 24개의 lag-1 변수 사용\n",
    "- 고객이 첫 계약 맺은 날짜(fecha_alta)와 고객이 마지막으로 1등급이었던 날짜(ult_fec_cli_lt)에서 연도와 월 정보 추출\n",
    "- 결측값은 임시로 -99로 대체\n",
    "- lag-N 변수는 N개월 전 보유 여부를 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 날짜 변수에서 연도와 월 정보를 추출\n",
    "df['fecha_alta_month'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['fecha_alta_year'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['fecha_alta_month', 'fecha_alta_year']\n",
    "df['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['ult_fec_cli_1t_month', 'ult_fec_cli_1t_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#결측 처리\n",
    "df.fillna(-99, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lag-1 데이터 생성하기\n",
    "\n",
    "# 날짜를 숫자로 변환: 2015-01-28은 1, 2016-06-28은 18로\n",
    "def date_to_int(str_date):\n",
    "    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")] \n",
    "    int_date = (int(Y) - 2015) * 12 + int(M)\n",
    "    return int_date\n",
    "\n",
    "df['int_date'] = df['fecha_dato'].map(date_to_int).astype(np.int8)\n",
    "\n",
    "df_lag = df.copy()\n",
    "df_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in df.columns ]\n",
    "df_lag['int_date'] += 1\n",
    "\n",
    "# 원본 데이터와 lag 데이터를 ncodper와 int_date 기준으로 통합\n",
    "df_trn = df.merge(df_lag, on=['ncodpers','int_date'], how='left')\n",
    "del df, df_lag #불필요한 데이터 제거\n",
    "\n",
    "# 저번 달의 제품 정보가 존재하지 않을 경우 0으로 대체\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    df_trn[prev].fillna(0, inplace=True)\n",
    "df_trn.fillna(-99, inplace=True)\n",
    "\n",
    "# 변수 리스트에 추가\n",
    "features += [feature + '_prev' for feature in features]\n",
    "features += [prod + '_prev' for prod in prods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델 학습** :\n",
    "- 교차검증: 그전 18개월 데이터가 주어지고 미래의 201606 데이터를 예측해야 하므로, 최신 데이터(201605)를 검증 데이터로 분리하고 나머지를 훈련으로 사용하는 것이 일반적, 일단 201601~201604 데이터를 훈련으로 사용. 평가척도인 MAP@7.\n",
    "- XGBoost\n",
    "- 검증데이터의 실제 정답값을 기반으로 MAP@7 계산하면 0.042663(최대값. 모든 고객이 신규구매를 하지 않았으므로)-이를 감안하여 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n",
    "trn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\n",
    "tst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\n",
    "del df_trn\n",
    "\n",
    "# 훈련 데이터에서 신규 구매 건수만 추출\n",
    "X = []\n",
    "Y = []\n",
    "for i, prod in enumerate(prods):\n",
    "    prev = prod + '_prev'\n",
    "    prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]\n",
    "    prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n",
    "    X.append(prX)\n",
    "    Y.append(prY)\n",
    "XY = pd.concat(X)\n",
    "Y = np.hstack(Y)\n",
    "XY['y'] = Y\n",
    "\n",
    "# 훈련, 검증 데이터로 분리\n",
    "vld_date = '2016-05-28'\n",
    "XY_trn = XY[XY['fecha_dato'] != vld_date]\n",
    "XY_vld = XY[XY['fecha_dato'] == vld_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yshin/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/Users/yshin/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/yshin/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/Users/yshin/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# 훈련, 검증 데이터를 XGBoost 형태로 변환\n",
    "X_trn = XY_trn.as_matrix(columns=features)\n",
    "Y_trn = XY_trn.as_matrix(columns=['y'])\n",
    "dtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)\n",
    "\n",
    "X_vld = XY_vld.as_matrix(columns=features)\n",
    "Y_vld = XY_vld.as_matrix(columns=['y'])\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
    "\n",
    "param = {\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': 8,\n",
    "    'nthread': 4,\n",
    "    'num_class': len(prods),\n",
    "    'objective': 'multi:softprob',\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'eta': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.9,\n",
    "    'seed': 2018,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.70304\teval-mlogloss:2.71262\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-mlogloss:2.46704\teval-mlogloss:2.47922\n",
      "[2]\ttrain-mlogloss:2.27876\teval-mlogloss:2.29281\n",
      "[3]\ttrain-mlogloss:2.14652\teval-mlogloss:2.16172\n",
      "[4]\ttrain-mlogloss:2.03258\teval-mlogloss:2.0492\n",
      "[5]\ttrain-mlogloss:1.9349\teval-mlogloss:1.95236\n",
      "[6]\ttrain-mlogloss:1.85608\teval-mlogloss:1.87393\n",
      "[7]\ttrain-mlogloss:1.78396\teval-mlogloss:1.80199\n",
      "[8]\ttrain-mlogloss:1.72348\teval-mlogloss:1.74199\n",
      "[9]\ttrain-mlogloss:1.66871\teval-mlogloss:1.68748\n",
      "[10]\ttrain-mlogloss:1.62359\teval-mlogloss:1.64315\n",
      "[11]\ttrain-mlogloss:1.58044\teval-mlogloss:1.60003\n",
      "[12]\ttrain-mlogloss:1.54032\teval-mlogloss:1.55999\n",
      "[13]\ttrain-mlogloss:1.50486\teval-mlogloss:1.52505\n",
      "[14]\ttrain-mlogloss:1.47283\teval-mlogloss:1.49331\n",
      "[15]\ttrain-mlogloss:1.44302\teval-mlogloss:1.46372\n",
      "[16]\ttrain-mlogloss:1.41661\teval-mlogloss:1.43781\n",
      "[17]\ttrain-mlogloss:1.39216\teval-mlogloss:1.41376\n",
      "[18]\ttrain-mlogloss:1.36987\teval-mlogloss:1.39191\n",
      "[19]\ttrain-mlogloss:1.35027\teval-mlogloss:1.37244\n",
      "[20]\ttrain-mlogloss:1.33107\teval-mlogloss:1.35342\n",
      "[21]\ttrain-mlogloss:1.31333\teval-mlogloss:1.33607\n",
      "[22]\ttrain-mlogloss:1.29718\teval-mlogloss:1.32022\n",
      "[23]\ttrain-mlogloss:1.28269\teval-mlogloss:1.30608\n",
      "[24]\ttrain-mlogloss:1.26888\teval-mlogloss:1.29251\n",
      "[25]\ttrain-mlogloss:1.25661\teval-mlogloss:1.28078\n",
      "[26]\ttrain-mlogloss:1.24462\teval-mlogloss:1.26911\n",
      "[27]\ttrain-mlogloss:1.2336\teval-mlogloss:1.25833\n",
      "[28]\ttrain-mlogloss:1.22323\teval-mlogloss:1.24828\n",
      "[29]\ttrain-mlogloss:1.21355\teval-mlogloss:1.2389\n",
      "[30]\ttrain-mlogloss:1.20466\teval-mlogloss:1.23052\n",
      "[31]\ttrain-mlogloss:1.19621\teval-mlogloss:1.22237\n",
      "[32]\ttrain-mlogloss:1.18843\teval-mlogloss:1.21504\n",
      "[33]\ttrain-mlogloss:1.1813\teval-mlogloss:1.20826\n",
      "[34]\ttrain-mlogloss:1.17456\teval-mlogloss:1.2018\n",
      "[35]\ttrain-mlogloss:1.16805\teval-mlogloss:1.1957\n",
      "[36]\ttrain-mlogloss:1.16227\teval-mlogloss:1.19034\n",
      "[37]\ttrain-mlogloss:1.15652\teval-mlogloss:1.18504\n",
      "[38]\ttrain-mlogloss:1.15096\teval-mlogloss:1.17994\n",
      "[39]\ttrain-mlogloss:1.14584\teval-mlogloss:1.1753\n",
      "[40]\ttrain-mlogloss:1.14081\teval-mlogloss:1.17064\n",
      "[41]\ttrain-mlogloss:1.1362\teval-mlogloss:1.16637\n",
      "[42]\ttrain-mlogloss:1.13173\teval-mlogloss:1.16231\n",
      "[43]\ttrain-mlogloss:1.12784\teval-mlogloss:1.1588\n",
      "[44]\ttrain-mlogloss:1.12396\teval-mlogloss:1.15532\n",
      "[45]\ttrain-mlogloss:1.12032\teval-mlogloss:1.15199\n",
      "[46]\ttrain-mlogloss:1.11658\teval-mlogloss:1.14877\n",
      "[47]\ttrain-mlogloss:1.11301\teval-mlogloss:1.14568\n",
      "[48]\ttrain-mlogloss:1.10972\teval-mlogloss:1.14289\n",
      "[49]\ttrain-mlogloss:1.10662\teval-mlogloss:1.14013\n",
      "[50]\ttrain-mlogloss:1.10371\teval-mlogloss:1.13764\n",
      "[51]\ttrain-mlogloss:1.10092\teval-mlogloss:1.13518\n",
      "[52]\ttrain-mlogloss:1.09826\teval-mlogloss:1.13291\n",
      "[53]\ttrain-mlogloss:1.0959\teval-mlogloss:1.13086\n",
      "[54]\ttrain-mlogloss:1.09343\teval-mlogloss:1.12884\n",
      "[55]\ttrain-mlogloss:1.09103\teval-mlogloss:1.12695\n",
      "[56]\ttrain-mlogloss:1.08887\teval-mlogloss:1.12528\n",
      "[57]\ttrain-mlogloss:1.0867\teval-mlogloss:1.12348\n",
      "[58]\ttrain-mlogloss:1.08463\teval-mlogloss:1.12183\n",
      "[59]\ttrain-mlogloss:1.08259\teval-mlogloss:1.12024\n",
      "[60]\ttrain-mlogloss:1.08075\teval-mlogloss:1.11877\n",
      "[61]\ttrain-mlogloss:1.07896\teval-mlogloss:1.11739\n",
      "[62]\ttrain-mlogloss:1.07718\teval-mlogloss:1.11601\n",
      "[63]\ttrain-mlogloss:1.07548\teval-mlogloss:1.11483\n",
      "[64]\ttrain-mlogloss:1.07388\teval-mlogloss:1.11364\n",
      "[65]\ttrain-mlogloss:1.07233\teval-mlogloss:1.11254\n",
      "[66]\ttrain-mlogloss:1.07082\teval-mlogloss:1.11142\n",
      "[67]\ttrain-mlogloss:1.06939\teval-mlogloss:1.11047\n",
      "[68]\ttrain-mlogloss:1.06796\teval-mlogloss:1.10951\n",
      "[69]\ttrain-mlogloss:1.06669\teval-mlogloss:1.10862\n",
      "[70]\ttrain-mlogloss:1.06539\teval-mlogloss:1.10787\n",
      "[71]\ttrain-mlogloss:1.0641\teval-mlogloss:1.10699\n",
      "[72]\ttrain-mlogloss:1.06286\teval-mlogloss:1.10616\n",
      "[73]\ttrain-mlogloss:1.06164\teval-mlogloss:1.10544\n",
      "[74]\ttrain-mlogloss:1.06055\teval-mlogloss:1.10477\n",
      "[75]\ttrain-mlogloss:1.05941\teval-mlogloss:1.10416\n",
      "[76]\ttrain-mlogloss:1.05832\teval-mlogloss:1.10352\n",
      "[77]\ttrain-mlogloss:1.05729\teval-mlogloss:1.10299\n",
      "[78]\ttrain-mlogloss:1.05638\teval-mlogloss:1.10242\n",
      "[79]\ttrain-mlogloss:1.05539\teval-mlogloss:1.10175\n",
      "[80]\ttrain-mlogloss:1.0544\teval-mlogloss:1.10117\n",
      "[81]\ttrain-mlogloss:1.05337\teval-mlogloss:1.1007\n",
      "[82]\ttrain-mlogloss:1.05231\teval-mlogloss:1.10022\n",
      "[83]\ttrain-mlogloss:1.05149\teval-mlogloss:1.09974\n",
      "[84]\ttrain-mlogloss:1.05054\teval-mlogloss:1.09922\n",
      "[85]\ttrain-mlogloss:1.0497\teval-mlogloss:1.09879\n",
      "[86]\ttrain-mlogloss:1.04893\teval-mlogloss:1.09836\n",
      "[87]\ttrain-mlogloss:1.04801\teval-mlogloss:1.09799\n",
      "[88]\ttrain-mlogloss:1.04722\teval-mlogloss:1.09762\n",
      "[89]\ttrain-mlogloss:1.0463\teval-mlogloss:1.09723\n",
      "[90]\ttrain-mlogloss:1.04548\teval-mlogloss:1.09689\n",
      "[91]\ttrain-mlogloss:1.04466\teval-mlogloss:1.09645\n",
      "[92]\ttrain-mlogloss:1.04395\teval-mlogloss:1.0961\n",
      "[93]\ttrain-mlogloss:1.0432\teval-mlogloss:1.09585\n",
      "[94]\ttrain-mlogloss:1.0425\teval-mlogloss:1.09551\n",
      "[95]\ttrain-mlogloss:1.04176\teval-mlogloss:1.0952\n",
      "[96]\ttrain-mlogloss:1.04097\teval-mlogloss:1.09493\n",
      "[97]\ttrain-mlogloss:1.04031\teval-mlogloss:1.09463\n",
      "[98]\ttrain-mlogloss:1.03966\teval-mlogloss:1.09447\n",
      "[99]\ttrain-mlogloss:1.03903\teval-mlogloss:1.09421\n",
      "[100]\ttrain-mlogloss:1.0383\teval-mlogloss:1.09395\n",
      "[101]\ttrain-mlogloss:1.03769\teval-mlogloss:1.09372\n",
      "[102]\ttrain-mlogloss:1.03689\teval-mlogloss:1.09348\n",
      "[103]\ttrain-mlogloss:1.03614\teval-mlogloss:1.09333\n",
      "[104]\ttrain-mlogloss:1.03554\teval-mlogloss:1.09315\n",
      "[105]\ttrain-mlogloss:1.03493\teval-mlogloss:1.09296\n",
      "[106]\ttrain-mlogloss:1.03444\teval-mlogloss:1.09278\n",
      "[107]\ttrain-mlogloss:1.0337\teval-mlogloss:1.09259\n",
      "[108]\ttrain-mlogloss:1.03302\teval-mlogloss:1.09243\n",
      "[109]\ttrain-mlogloss:1.03239\teval-mlogloss:1.09224\n",
      "[110]\ttrain-mlogloss:1.03174\teval-mlogloss:1.09214\n",
      "[111]\ttrain-mlogloss:1.03112\teval-mlogloss:1.09199\n",
      "[112]\ttrain-mlogloss:1.03035\teval-mlogloss:1.09184\n",
      "[113]\ttrain-mlogloss:1.02967\teval-mlogloss:1.09173\n",
      "[114]\ttrain-mlogloss:1.02904\teval-mlogloss:1.09156\n",
      "[115]\ttrain-mlogloss:1.02845\teval-mlogloss:1.0914\n",
      "[116]\ttrain-mlogloss:1.02762\teval-mlogloss:1.09127\n",
      "[117]\ttrain-mlogloss:1.02691\teval-mlogloss:1.09107\n",
      "[118]\ttrain-mlogloss:1.02618\teval-mlogloss:1.09097\n",
      "[119]\ttrain-mlogloss:1.02561\teval-mlogloss:1.09082\n",
      "[120]\ttrain-mlogloss:1.02496\teval-mlogloss:1.09068\n",
      "[121]\ttrain-mlogloss:1.02435\teval-mlogloss:1.0906\n",
      "[122]\ttrain-mlogloss:1.02367\teval-mlogloss:1.09046\n",
      "[123]\ttrain-mlogloss:1.02315\teval-mlogloss:1.09037\n",
      "[124]\ttrain-mlogloss:1.02262\teval-mlogloss:1.09026\n",
      "[125]\ttrain-mlogloss:1.022\teval-mlogloss:1.09019\n",
      "[126]\ttrain-mlogloss:1.02118\teval-mlogloss:1.09005\n",
      "[127]\ttrain-mlogloss:1.02064\teval-mlogloss:1.08997\n",
      "[128]\ttrain-mlogloss:1.02005\teval-mlogloss:1.08991\n",
      "[129]\ttrain-mlogloss:1.01947\teval-mlogloss:1.08985\n",
      "[130]\ttrain-mlogloss:1.01885\teval-mlogloss:1.08976\n",
      "[131]\ttrain-mlogloss:1.01829\teval-mlogloss:1.08969\n",
      "[132]\ttrain-mlogloss:1.01767\teval-mlogloss:1.08964\n",
      "[133]\ttrain-mlogloss:1.01699\teval-mlogloss:1.08958\n",
      "[134]\ttrain-mlogloss:1.01633\teval-mlogloss:1.0895\n",
      "[135]\ttrain-mlogloss:1.0157\teval-mlogloss:1.08943\n",
      "[136]\ttrain-mlogloss:1.01502\teval-mlogloss:1.08941\n",
      "[137]\ttrain-mlogloss:1.01461\teval-mlogloss:1.08935\n",
      "[138]\ttrain-mlogloss:1.01395\teval-mlogloss:1.08927\n",
      "[139]\ttrain-mlogloss:1.01326\teval-mlogloss:1.08919\n",
      "[140]\ttrain-mlogloss:1.01269\teval-mlogloss:1.08909\n",
      "[141]\ttrain-mlogloss:1.01199\teval-mlogloss:1.08904\n",
      "[142]\ttrain-mlogloss:1.01139\teval-mlogloss:1.08902\n",
      "[143]\ttrain-mlogloss:1.01077\teval-mlogloss:1.08895\n",
      "[144]\ttrain-mlogloss:1.01022\teval-mlogloss:1.08889\n",
      "[145]\ttrain-mlogloss:1.00958\teval-mlogloss:1.08885\n",
      "[146]\ttrain-mlogloss:1.00901\teval-mlogloss:1.08883\n",
      "[147]\ttrain-mlogloss:1.00847\teval-mlogloss:1.08878\n",
      "[148]\ttrain-mlogloss:1.00794\teval-mlogloss:1.08875\n",
      "[149]\ttrain-mlogloss:1.00744\teval-mlogloss:1.08869\n",
      "[150]\ttrain-mlogloss:1.00678\teval-mlogloss:1.0887\n",
      "[151]\ttrain-mlogloss:1.00616\teval-mlogloss:1.08874\n",
      "[152]\ttrain-mlogloss:1.00553\teval-mlogloss:1.08867\n",
      "[153]\ttrain-mlogloss:1.00482\teval-mlogloss:1.0886\n",
      "[154]\ttrain-mlogloss:1.00421\teval-mlogloss:1.08858\n",
      "[155]\ttrain-mlogloss:1.00349\teval-mlogloss:1.08849\n",
      "[156]\ttrain-mlogloss:1.00281\teval-mlogloss:1.08845\n",
      "[157]\ttrain-mlogloss:1.00208\teval-mlogloss:1.08841\n",
      "[158]\ttrain-mlogloss:1.00153\teval-mlogloss:1.08834\n",
      "[159]\ttrain-mlogloss:1.00085\teval-mlogloss:1.08828\n",
      "[160]\ttrain-mlogloss:1.00013\teval-mlogloss:1.08819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161]\ttrain-mlogloss:0.999602\teval-mlogloss:1.08823\n",
      "[162]\ttrain-mlogloss:0.999053\teval-mlogloss:1.08823\n",
      "[163]\ttrain-mlogloss:0.998557\teval-mlogloss:1.08825\n",
      "[164]\ttrain-mlogloss:0.997948\teval-mlogloss:1.08826\n",
      "[165]\ttrain-mlogloss:0.997319\teval-mlogloss:1.08823\n",
      "[166]\ttrain-mlogloss:0.996664\teval-mlogloss:1.08816\n",
      "[167]\ttrain-mlogloss:0.996206\teval-mlogloss:1.08819\n",
      "[168]\ttrain-mlogloss:0.995539\teval-mlogloss:1.08819\n",
      "[169]\ttrain-mlogloss:0.994933\teval-mlogloss:1.0882\n",
      "[170]\ttrain-mlogloss:0.994335\teval-mlogloss:1.08821\n",
      "[171]\ttrain-mlogloss:0.993719\teval-mlogloss:1.0882\n",
      "[172]\ttrain-mlogloss:0.993045\teval-mlogloss:1.08824\n",
      "[173]\ttrain-mlogloss:0.992507\teval-mlogloss:1.08823\n",
      "[174]\ttrain-mlogloss:0.991895\teval-mlogloss:1.08815\n",
      "[175]\ttrain-mlogloss:0.991339\teval-mlogloss:1.08813\n",
      "[176]\ttrain-mlogloss:0.990784\teval-mlogloss:1.08815\n",
      "[177]\ttrain-mlogloss:0.990279\teval-mlogloss:1.08815\n",
      "[178]\ttrain-mlogloss:0.989619\teval-mlogloss:1.08812\n",
      "[179]\ttrain-mlogloss:0.989198\teval-mlogloss:1.08815\n",
      "[180]\ttrain-mlogloss:0.988753\teval-mlogloss:1.08813\n",
      "[181]\ttrain-mlogloss:0.988119\teval-mlogloss:1.08807\n",
      "[182]\ttrain-mlogloss:0.987415\teval-mlogloss:1.08803\n",
      "[183]\ttrain-mlogloss:0.986963\teval-mlogloss:1.08803\n",
      "[184]\ttrain-mlogloss:0.98633\teval-mlogloss:1.08796\n",
      "[185]\ttrain-mlogloss:0.9858\teval-mlogloss:1.088\n",
      "[186]\ttrain-mlogloss:0.985099\teval-mlogloss:1.08794\n",
      "[187]\ttrain-mlogloss:0.984715\teval-mlogloss:1.08793\n",
      "[188]\ttrain-mlogloss:0.984232\teval-mlogloss:1.08787\n",
      "[189]\ttrain-mlogloss:0.9838\teval-mlogloss:1.08784\n",
      "[190]\ttrain-mlogloss:0.983187\teval-mlogloss:1.08779\n",
      "[191]\ttrain-mlogloss:0.982675\teval-mlogloss:1.0878\n",
      "[192]\ttrain-mlogloss:0.982187\teval-mlogloss:1.0878\n",
      "[193]\ttrain-mlogloss:0.98161\teval-mlogloss:1.0878\n",
      "[194]\ttrain-mlogloss:0.981162\teval-mlogloss:1.0878\n",
      "[195]\ttrain-mlogloss:0.980779\teval-mlogloss:1.08783\n",
      "[196]\ttrain-mlogloss:0.980203\teval-mlogloss:1.08784\n",
      "[197]\ttrain-mlogloss:0.979627\teval-mlogloss:1.0878\n",
      "[198]\ttrain-mlogloss:0.979146\teval-mlogloss:1.08781\n",
      "[199]\ttrain-mlogloss:0.978548\teval-mlogloss:1.08776\n",
      "[200]\ttrain-mlogloss:0.97799\teval-mlogloss:1.08769\n",
      "[201]\ttrain-mlogloss:0.977445\teval-mlogloss:1.08767\n",
      "[202]\ttrain-mlogloss:0.977064\teval-mlogloss:1.08765\n",
      "[203]\ttrain-mlogloss:0.97636\teval-mlogloss:1.08764\n",
      "[204]\ttrain-mlogloss:0.975788\teval-mlogloss:1.08758\n",
      "[205]\ttrain-mlogloss:0.975311\teval-mlogloss:1.08759\n",
      "[206]\ttrain-mlogloss:0.97484\teval-mlogloss:1.08752\n",
      "[207]\ttrain-mlogloss:0.974299\teval-mlogloss:1.08749\n",
      "[208]\ttrain-mlogloss:0.973711\teval-mlogloss:1.08746\n",
      "[209]\ttrain-mlogloss:0.973104\teval-mlogloss:1.08742\n",
      "[210]\ttrain-mlogloss:0.972499\teval-mlogloss:1.08741\n",
      "[211]\ttrain-mlogloss:0.972008\teval-mlogloss:1.0874\n",
      "[212]\ttrain-mlogloss:0.971502\teval-mlogloss:1.0874\n",
      "[213]\ttrain-mlogloss:0.971078\teval-mlogloss:1.08736\n",
      "[214]\ttrain-mlogloss:0.9705\teval-mlogloss:1.08738\n",
      "[215]\ttrain-mlogloss:0.969948\teval-mlogloss:1.08736\n",
      "[216]\ttrain-mlogloss:0.969408\teval-mlogloss:1.08737\n",
      "[217]\ttrain-mlogloss:0.968918\teval-mlogloss:1.08743\n",
      "[218]\ttrain-mlogloss:0.9685\teval-mlogloss:1.08742\n",
      "[219]\ttrain-mlogloss:0.968048\teval-mlogloss:1.08745\n",
      "[220]\ttrain-mlogloss:0.967457\teval-mlogloss:1.08744\n",
      "[221]\ttrain-mlogloss:0.966922\teval-mlogloss:1.08748\n",
      "[222]\ttrain-mlogloss:0.966492\teval-mlogloss:1.08749\n",
      "[223]\ttrain-mlogloss:0.966071\teval-mlogloss:1.08754\n",
      "[224]\ttrain-mlogloss:0.965608\teval-mlogloss:1.08754\n",
      "[225]\ttrain-mlogloss:0.965126\teval-mlogloss:1.0875\n",
      "[226]\ttrain-mlogloss:0.964466\teval-mlogloss:1.08751\n",
      "[227]\ttrain-mlogloss:0.963871\teval-mlogloss:1.08744\n",
      "[228]\ttrain-mlogloss:0.963178\teval-mlogloss:1.08733\n",
      "[229]\ttrain-mlogloss:0.962681\teval-mlogloss:1.08733\n",
      "[230]\ttrain-mlogloss:0.962267\teval-mlogloss:1.08733\n",
      "[231]\ttrain-mlogloss:0.961805\teval-mlogloss:1.0873\n",
      "[232]\ttrain-mlogloss:0.961282\teval-mlogloss:1.08727\n",
      "[233]\ttrain-mlogloss:0.960744\teval-mlogloss:1.08727\n",
      "[234]\ttrain-mlogloss:0.960278\teval-mlogloss:1.08727\n",
      "[235]\ttrain-mlogloss:0.95977\teval-mlogloss:1.08729\n",
      "[236]\ttrain-mlogloss:0.95927\teval-mlogloss:1.08728\n",
      "[237]\ttrain-mlogloss:0.958798\teval-mlogloss:1.08735\n",
      "[238]\ttrain-mlogloss:0.958313\teval-mlogloss:1.08738\n",
      "[239]\ttrain-mlogloss:0.957838\teval-mlogloss:1.08736\n",
      "[240]\ttrain-mlogloss:0.957381\teval-mlogloss:1.08736\n",
      "[241]\ttrain-mlogloss:0.956874\teval-mlogloss:1.08737\n",
      "[242]\ttrain-mlogloss:0.956379\teval-mlogloss:1.08737\n",
      "[243]\ttrain-mlogloss:0.955869\teval-mlogloss:1.08735\n",
      "[244]\ttrain-mlogloss:0.955395\teval-mlogloss:1.08736\n",
      "[245]\ttrain-mlogloss:0.955024\teval-mlogloss:1.08734\n",
      "[246]\ttrain-mlogloss:0.954562\teval-mlogloss:1.08735\n",
      "[247]\ttrain-mlogloss:0.954122\teval-mlogloss:1.08738\n",
      "[248]\ttrain-mlogloss:0.953546\teval-mlogloss:1.08738\n",
      "[249]\ttrain-mlogloss:0.953106\teval-mlogloss:1.0874\n",
      "[250]\ttrain-mlogloss:0.952614\teval-mlogloss:1.0874\n",
      "[251]\ttrain-mlogloss:0.95218\teval-mlogloss:1.08738\n",
      "[252]\ttrain-mlogloss:0.951686\teval-mlogloss:1.08744\n",
      "[253]\ttrain-mlogloss:0.951223\teval-mlogloss:1.08745\n",
      "Stopping. Best iteration:\n",
      "[233]\ttrain-mlogloss:0.960744\teval-mlogloss:1.08727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#훈련 데이터 학습\n",
    "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
    "model = xgb.train(param, dtrn, num_boost_round=1000, evals=watch_list, early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 저장\n",
    "best_ntree_limit = model.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yshin/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/yshin/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/yshin/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# 고객 식별 번호를 추출\n",
    "vld = trn[trn['fecha_dato'] == vld_date]\n",
    "ncodpers_vld = vld.as_matrix(columns=['ncodpers'])\n",
    "# 검증 데이터에서 신규 구매\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    padd = prod + '_add'\n",
    "    vld[padd] = vld[prod] - vld[prev]    \n",
    "add_vld = vld.as_matrix(columns=[prod + '_add' for prod in prods])\n",
    "add_vld_list = [list() for i in range(len(ncodpers_vld))]\n",
    "\n",
    "# 고객별 신규 구매 정답 값을 add_vld_list에, 총 count를 count_vld에 저장\n",
    "count_vld = 0\n",
    "for ncodper in range(len(ncodpers_vld)):\n",
    "    for prod in range(len(prods)):\n",
    "        if add_vld[ncodper, prod] > 0:\n",
    "            add_vld_list[ncodper].append(prod)\n",
    "            count_vld += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04266379915553903\n"
     ]
    }
   ],
   "source": [
    "#MAP@7\n",
    "def apk(actual, predicted, k=7, default=0.0):\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "    # 정답값이 공백일 경우, 무조건 0.0점을 반환\n",
    "    if not actual:\n",
    "        return default\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=7, default=0.0):\n",
    "    # list of list인 정답값(actual)과 예측값(predicted)에서 고객별 Average Precision을 구하고, np.mean()을 통해 평균을 계산\n",
    "    return np.mean([apk(a, p, k, default) for a, p in zip(actual, predicted)]) \n",
    "\n",
    "# 검증 데이터에서 얻을 수 있는 MAP@7 최고점\n",
    "print(mapk(add_vld_list, add_vld_list, 7, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yshin/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/Users/yshin/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.6967\n",
      "[1]\ttrain-mlogloss:2.44469\n",
      "[2]\ttrain-mlogloss:2.27264\n",
      "[3]\ttrain-mlogloss:2.1345\n",
      "[4]\ttrain-mlogloss:2.02023\n",
      "[5]\ttrain-mlogloss:1.92685\n",
      "[6]\ttrain-mlogloss:1.84516\n",
      "[7]\ttrain-mlogloss:1.77828\n",
      "[8]\ttrain-mlogloss:1.71895\n",
      "[9]\ttrain-mlogloss:1.66587\n",
      "[10]\ttrain-mlogloss:1.61836\n",
      "[11]\ttrain-mlogloss:1.57639\n",
      "[12]\ttrain-mlogloss:1.53857\n",
      "[13]\ttrain-mlogloss:1.50259\n",
      "[14]\ttrain-mlogloss:1.47056\n",
      "[15]\ttrain-mlogloss:1.44218\n",
      "[16]\ttrain-mlogloss:1.41604\n",
      "[17]\ttrain-mlogloss:1.39154\n",
      "[18]\ttrain-mlogloss:1.36921\n",
      "[19]\ttrain-mlogloss:1.34925\n",
      "[20]\ttrain-mlogloss:1.33049\n",
      "[21]\ttrain-mlogloss:1.31304\n",
      "[22]\ttrain-mlogloss:1.29741\n",
      "[23]\ttrain-mlogloss:1.28297\n",
      "[24]\ttrain-mlogloss:1.26929\n",
      "[25]\ttrain-mlogloss:1.25651\n",
      "[26]\ttrain-mlogloss:1.24486\n",
      "[27]\ttrain-mlogloss:1.23378\n",
      "[28]\ttrain-mlogloss:1.22347\n",
      "[29]\ttrain-mlogloss:1.21412\n",
      "[30]\ttrain-mlogloss:1.20526\n",
      "[31]\ttrain-mlogloss:1.1969\n",
      "[32]\ttrain-mlogloss:1.18917\n",
      "[33]\ttrain-mlogloss:1.18208\n",
      "[34]\ttrain-mlogloss:1.1753\n",
      "[35]\ttrain-mlogloss:1.16866\n",
      "[36]\ttrain-mlogloss:1.16234\n",
      "[37]\ttrain-mlogloss:1.1567\n",
      "[38]\ttrain-mlogloss:1.1511\n",
      "[39]\ttrain-mlogloss:1.14597\n",
      "[40]\ttrain-mlogloss:1.14119\n",
      "[41]\ttrain-mlogloss:1.13653\n",
      "[42]\ttrain-mlogloss:1.13209\n",
      "[43]\ttrain-mlogloss:1.12787\n",
      "[44]\ttrain-mlogloss:1.12394\n",
      "[45]\ttrain-mlogloss:1.12004\n",
      "[46]\ttrain-mlogloss:1.11651\n",
      "[47]\ttrain-mlogloss:1.11292\n",
      "[48]\ttrain-mlogloss:1.10989\n",
      "[49]\ttrain-mlogloss:1.10681\n",
      "[50]\ttrain-mlogloss:1.10389\n",
      "[51]\ttrain-mlogloss:1.10116\n",
      "[52]\ttrain-mlogloss:1.0986\n",
      "[53]\ttrain-mlogloss:1.09612\n",
      "[54]\ttrain-mlogloss:1.09383\n",
      "[55]\ttrain-mlogloss:1.09165\n",
      "[56]\ttrain-mlogloss:1.08945\n",
      "[57]\ttrain-mlogloss:1.08736\n",
      "[58]\ttrain-mlogloss:1.08542\n",
      "[59]\ttrain-mlogloss:1.08354\n",
      "[60]\ttrain-mlogloss:1.08168\n",
      "[61]\ttrain-mlogloss:1.07992\n",
      "[62]\ttrain-mlogloss:1.07819\n",
      "[63]\ttrain-mlogloss:1.07656\n",
      "[64]\ttrain-mlogloss:1.07497\n",
      "[65]\ttrain-mlogloss:1.07342\n",
      "[66]\ttrain-mlogloss:1.07187\n",
      "[67]\ttrain-mlogloss:1.07042\n",
      "[68]\ttrain-mlogloss:1.06913\n",
      "[69]\ttrain-mlogloss:1.06784\n",
      "[70]\ttrain-mlogloss:1.06652\n",
      "[71]\ttrain-mlogloss:1.0654\n",
      "[72]\ttrain-mlogloss:1.06411\n",
      "[73]\ttrain-mlogloss:1.06305\n",
      "[74]\ttrain-mlogloss:1.06198\n",
      "[75]\ttrain-mlogloss:1.06078\n",
      "[76]\ttrain-mlogloss:1.05965\n",
      "[77]\ttrain-mlogloss:1.05872\n",
      "[78]\ttrain-mlogloss:1.05779\n",
      "[79]\ttrain-mlogloss:1.0568\n",
      "[80]\ttrain-mlogloss:1.05583\n",
      "[81]\ttrain-mlogloss:1.05488\n",
      "[82]\ttrain-mlogloss:1.0541\n",
      "[83]\ttrain-mlogloss:1.05327\n",
      "[84]\ttrain-mlogloss:1.05243\n",
      "[85]\ttrain-mlogloss:1.05155\n",
      "[86]\ttrain-mlogloss:1.05086\n",
      "[87]\ttrain-mlogloss:1.05006\n",
      "[88]\ttrain-mlogloss:1.04929\n",
      "[89]\ttrain-mlogloss:1.04855\n",
      "[90]\ttrain-mlogloss:1.04783\n",
      "[91]\ttrain-mlogloss:1.04708\n",
      "[92]\ttrain-mlogloss:1.04641\n",
      "[93]\ttrain-mlogloss:1.04572\n",
      "[94]\ttrain-mlogloss:1.0451\n",
      "[95]\ttrain-mlogloss:1.04442\n",
      "[96]\ttrain-mlogloss:1.04378\n",
      "[97]\ttrain-mlogloss:1.04316\n",
      "[98]\ttrain-mlogloss:1.04242\n",
      "[99]\ttrain-mlogloss:1.04182\n",
      "[100]\ttrain-mlogloss:1.04129\n",
      "[101]\ttrain-mlogloss:1.04058\n",
      "[102]\ttrain-mlogloss:1.03988\n",
      "[103]\ttrain-mlogloss:1.03929\n",
      "[104]\ttrain-mlogloss:1.03861\n",
      "[105]\ttrain-mlogloss:1.03806\n",
      "[106]\ttrain-mlogloss:1.0374\n",
      "[107]\ttrain-mlogloss:1.03691\n",
      "[108]\ttrain-mlogloss:1.0363\n",
      "[109]\ttrain-mlogloss:1.03575\n",
      "[110]\ttrain-mlogloss:1.03524\n",
      "[111]\ttrain-mlogloss:1.03469\n",
      "[112]\ttrain-mlogloss:1.0341\n",
      "[113]\ttrain-mlogloss:1.03366\n",
      "[114]\ttrain-mlogloss:1.03317\n",
      "[115]\ttrain-mlogloss:1.03268\n",
      "[116]\ttrain-mlogloss:1.0321\n",
      "[117]\ttrain-mlogloss:1.03155\n",
      "[118]\ttrain-mlogloss:1.03104\n",
      "[119]\ttrain-mlogloss:1.03052\n",
      "[120]\ttrain-mlogloss:1.03001\n",
      "[121]\ttrain-mlogloss:1.02947\n",
      "[122]\ttrain-mlogloss:1.02913\n",
      "[123]\ttrain-mlogloss:1.02866\n",
      "[124]\ttrain-mlogloss:1.02801\n",
      "[125]\ttrain-mlogloss:1.02753\n",
      "[126]\ttrain-mlogloss:1.0271\n",
      "[127]\ttrain-mlogloss:1.02655\n",
      "[128]\ttrain-mlogloss:1.02603\n",
      "[129]\ttrain-mlogloss:1.02556\n",
      "[130]\ttrain-mlogloss:1.02481\n",
      "[131]\ttrain-mlogloss:1.02431\n",
      "[132]\ttrain-mlogloss:1.02379\n",
      "[133]\ttrain-mlogloss:1.02339\n",
      "[134]\ttrain-mlogloss:1.02295\n",
      "[135]\ttrain-mlogloss:1.02252\n",
      "[136]\ttrain-mlogloss:1.02208\n",
      "[137]\ttrain-mlogloss:1.02148\n",
      "[138]\ttrain-mlogloss:1.0209\n",
      "[139]\ttrain-mlogloss:1.02032\n",
      "[140]\ttrain-mlogloss:1.01978\n",
      "[141]\ttrain-mlogloss:1.01932\n",
      "[142]\ttrain-mlogloss:1.01889\n",
      "[143]\ttrain-mlogloss:1.01843\n",
      "[144]\ttrain-mlogloss:1.01794\n",
      "[145]\ttrain-mlogloss:1.01746\n",
      "[146]\ttrain-mlogloss:1.01697\n",
      "[147]\ttrain-mlogloss:1.01658\n",
      "[148]\ttrain-mlogloss:1.01606\n",
      "[149]\ttrain-mlogloss:1.01566\n",
      "[150]\ttrain-mlogloss:1.01514\n",
      "[151]\ttrain-mlogloss:1.01468\n",
      "[152]\ttrain-mlogloss:1.01414\n",
      "[153]\ttrain-mlogloss:1.01358\n",
      "[154]\ttrain-mlogloss:1.01303\n",
      "[155]\ttrain-mlogloss:1.01237\n",
      "[156]\ttrain-mlogloss:1.01192\n",
      "[157]\ttrain-mlogloss:1.01139\n",
      "[158]\ttrain-mlogloss:1.01083\n",
      "[159]\ttrain-mlogloss:1.01048\n",
      "[160]\ttrain-mlogloss:1.01008\n",
      "[161]\ttrain-mlogloss:1.00957\n",
      "[162]\ttrain-mlogloss:1.00911\n",
      "[163]\ttrain-mlogloss:1.00866\n",
      "[164]\ttrain-mlogloss:1.00802\n",
      "[165]\ttrain-mlogloss:1.00768\n",
      "[166]\ttrain-mlogloss:1.00714\n",
      "[167]\ttrain-mlogloss:1.00666\n",
      "[168]\ttrain-mlogloss:1.00608\n",
      "[169]\ttrain-mlogloss:1.00557\n",
      "[170]\ttrain-mlogloss:1.00514\n",
      "[171]\ttrain-mlogloss:1.00477\n",
      "[172]\ttrain-mlogloss:1.00433\n",
      "[173]\ttrain-mlogloss:1.00384\n",
      "[174]\ttrain-mlogloss:1.0034\n",
      "[175]\ttrain-mlogloss:1.00288\n",
      "[176]\ttrain-mlogloss:1.00235\n",
      "[177]\ttrain-mlogloss:1.00184\n",
      "[178]\ttrain-mlogloss:1.00135\n",
      "[179]\ttrain-mlogloss:1.00079\n",
      "[180]\ttrain-mlogloss:1.00039\n",
      "[181]\ttrain-mlogloss:0.999869\n",
      "[182]\ttrain-mlogloss:0.999469\n",
      "[183]\ttrain-mlogloss:0.998942\n",
      "[184]\ttrain-mlogloss:0.998433\n",
      "[185]\ttrain-mlogloss:0.997938\n",
      "[186]\ttrain-mlogloss:0.997436\n",
      "[187]\ttrain-mlogloss:0.996875\n",
      "[188]\ttrain-mlogloss:0.99652\n",
      "[189]\ttrain-mlogloss:0.996051\n",
      "[190]\ttrain-mlogloss:0.995508\n",
      "[191]\ttrain-mlogloss:0.99494\n",
      "[192]\ttrain-mlogloss:0.994488\n",
      "[193]\ttrain-mlogloss:0.993985\n",
      "[194]\ttrain-mlogloss:0.99339\n",
      "[195]\ttrain-mlogloss:0.992781\n",
      "[196]\ttrain-mlogloss:0.992365\n",
      "[197]\ttrain-mlogloss:0.991886\n",
      "[198]\ttrain-mlogloss:0.991466\n",
      "[199]\ttrain-mlogloss:0.990943\n",
      "[200]\ttrain-mlogloss:0.990437\n",
      "[201]\ttrain-mlogloss:0.989843\n",
      "[202]\ttrain-mlogloss:0.989406\n",
      "[203]\ttrain-mlogloss:0.988944\n",
      "[204]\ttrain-mlogloss:0.988507\n",
      "[205]\ttrain-mlogloss:0.988012\n",
      "[206]\ttrain-mlogloss:0.987602\n",
      "[207]\ttrain-mlogloss:0.986957\n",
      "[208]\ttrain-mlogloss:0.986416\n",
      "[209]\ttrain-mlogloss:0.985953\n",
      "[210]\ttrain-mlogloss:0.985449\n",
      "[211]\ttrain-mlogloss:0.985015\n",
      "[212]\ttrain-mlogloss:0.984466\n",
      "[213]\ttrain-mlogloss:0.983939\n",
      "[214]\ttrain-mlogloss:0.983295\n",
      "[215]\ttrain-mlogloss:0.982842\n",
      "[216]\ttrain-mlogloss:0.982324\n",
      "[217]\ttrain-mlogloss:0.981836\n",
      "[218]\ttrain-mlogloss:0.981389\n",
      "[219]\ttrain-mlogloss:0.980912\n",
      "[220]\ttrain-mlogloss:0.98047\n",
      "[221]\ttrain-mlogloss:0.98003\n",
      "[222]\ttrain-mlogloss:0.979394\n",
      "[223]\ttrain-mlogloss:0.978922\n",
      "[224]\ttrain-mlogloss:0.978372\n",
      "[225]\ttrain-mlogloss:0.977799\n",
      "[226]\ttrain-mlogloss:0.977253\n",
      "[227]\ttrain-mlogloss:0.976803\n",
      "[228]\ttrain-mlogloss:0.976296\n",
      "[229]\ttrain-mlogloss:0.975794\n",
      "[230]\ttrain-mlogloss:0.975196\n",
      "[231]\ttrain-mlogloss:0.974674\n",
      "[232]\ttrain-mlogloss:0.974265\n",
      "[233]\ttrain-mlogloss:0.97383\n",
      "[234]\ttrain-mlogloss:0.973358\n",
      "[235]\ttrain-mlogloss:0.972886\n",
      "[236]\ttrain-mlogloss:0.972549\n",
      "[237]\ttrain-mlogloss:0.972038\n",
      "[238]\ttrain-mlogloss:0.971601\n",
      "[239]\ttrain-mlogloss:0.971139\n",
      "[240]\ttrain-mlogloss:0.970652\n",
      "[241]\ttrain-mlogloss:0.970154\n",
      "[242]\ttrain-mlogloss:0.969743\n",
      "[243]\ttrain-mlogloss:0.969178\n",
      "[244]\ttrain-mlogloss:0.968852\n",
      "[245]\ttrain-mlogloss:0.968507\n",
      "[246]\ttrain-mlogloss:0.968078\n",
      "[247]\ttrain-mlogloss:0.967749\n",
      "[248]\ttrain-mlogloss:0.96732\n",
      "[249]\ttrain-mlogloss:0.966951\n",
      "[250]\ttrain-mlogloss:0.966576\n",
      "[251]\ttrain-mlogloss:0.966092\n",
      "[252]\ttrain-mlogloss:0.965671\n",
      "[253]\ttrain-mlogloss:0.965276\n",
      "[254]\ttrain-mlogloss:0.964744\n",
      "[255]\ttrain-mlogloss:0.964445\n",
      "[256]\ttrain-mlogloss:0.964009\n",
      "[257]\ttrain-mlogloss:0.963548\n",
      "[258]\ttrain-mlogloss:0.963123\n",
      "[259]\ttrain-mlogloss:0.962736\n",
      "[260]\ttrain-mlogloss:0.962312\n",
      "[261]\ttrain-mlogloss:0.961836\n",
      "[262]\ttrain-mlogloss:0.961313\n",
      "[263]\ttrain-mlogloss:0.960851\n",
      "[264]\ttrain-mlogloss:0.960442\n",
      "[265]\ttrain-mlogloss:0.959982\n",
      "[266]\ttrain-mlogloss:0.959446\n",
      "[267]\ttrain-mlogloss:0.958979\n",
      "[268]\ttrain-mlogloss:0.958594\n",
      "[269]\ttrain-mlogloss:0.958096\n",
      "[270]\ttrain-mlogloss:0.957746\n",
      "[271]\ttrain-mlogloss:0.957346\n",
      "[272]\ttrain-mlogloss:0.956962\n",
      "[273]\ttrain-mlogloss:0.956553\n",
      "[274]\ttrain-mlogloss:0.956059\n",
      "[275]\ttrain-mlogloss:0.955715\n",
      "[276]\ttrain-mlogloss:0.955379\n",
      "[277]\ttrain-mlogloss:0.955006\n",
      "[278]\ttrain-mlogloss:0.954666\n",
      "[279]\ttrain-mlogloss:0.954293\n",
      "[280]\ttrain-mlogloss:0.953856\n",
      "[281]\ttrain-mlogloss:0.953425\n",
      "[282]\ttrain-mlogloss:0.953125\n",
      "[283]\ttrain-mlogloss:0.9527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[284]\ttrain-mlogloss:0.952272\n",
      "[285]\ttrain-mlogloss:0.951937\n",
      "[286]\ttrain-mlogloss:0.951556\n",
      "[287]\ttrain-mlogloss:0.951189\n",
      "[288]\ttrain-mlogloss:0.950769\n"
     ]
    }
   ],
   "source": [
    "# 전체 훈련 데이터\n",
    "X_all = XY.as_matrix(columns=features)\n",
    "Y_all = XY.as_matrix(columns=['y'])\n",
    "dall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\n",
    "watch_list = [(dall, 'train')]\n",
    "# 트리 개수를 늘어난 데이터 양만큼 비례해서 증가\n",
    "best_ntree_limit = int(best_ntree_limit * (len(XY_trn) + len(XY_vld)) / len(XY_trn))\n",
    "# 재학습!\n",
    "model = xgb.train(param, dall, num_boost_round=best_ntree_limit, evals=watch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yshin/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 예측\n",
    "X_tst = tst.as_matrix(columns=features)\n",
    "dtst = xgb.DMatrix(X_tst, feature_names=features)\n",
    "preds_tst = model.predict(dtst, ntree_limit=best_ntree_limit)\n",
    "ncodpers_tst = tst.as_matrix(columns=['ncodpers'])\n",
    "preds_tst = preds_tst - tst.as_matrix(columns=[prod + '_prev' for prod in prods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
